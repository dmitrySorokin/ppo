{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Proximal Policy Optimization \n",
    "\n",
    "\n",
    "In this notebook you will be implementing Proximal Policy Optimization algorithm, \n",
    "scaled up version of which was used to train [OpenAI Five](https://openai.com/blog/openai-five/) \n",
    "to [win](https://openai.com/blog/how-to-train-your-openai-five/) against the\n",
    "world champions in Dota 2.\n",
    "You will be solving a continuous control environment on which it may be easier and faster \n",
    "to train an agent, however note that PPO here may not be the best algorithm as, for example,\n",
    "Deep Deterministic Policy Gradient and Soft Actor Critic may be more suited \n",
    "for continuous control environments. To run the environment you will need to install \n",
    "[pybullet-gym](https://github.com/benelot/pybullet-gym) which unlike MuJoCo \n",
    "does not require you to have a license.\n",
    "\n",
    "To install the library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall structure of the code is similar to the one in the A2C optional homework, but don't worry if you haven't done it, it should be relatively easy to figure it out. \n",
    "First, we will create an instance of the environment. \n",
    "We will normalize the observations and rewards, but before that you will need a wrapper that will \n",
    "write summaries, mainly, the total reward during an episode. You can either use one for `TensorFlow` \n",
    "implemented in `atari_wrappers.py` file from the optional A2C homework, or implement your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8:\n",
      "Process Process-7:\n",
      "Process Process-2:\n",
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-5:\n",
      "Process Process-4:\n",
      "Process Process-3:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_interf\n",
    "from env_batch import ParallelEnvBatch\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def make_interf_env(seed):\n",
    "    env = gym.make('interf-v1')\n",
    "    env.set_calc_reward('delta_visib')\n",
    "    #env.set_calc_image('gpu')\n",
    "    env.seed(seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_env(nenvs):\n",
    "    seed = list(range(nenvs))\n",
    "    env = ParallelEnvBatch([\n",
    "        lambda env_seed=env_seed: make_interf_env(seed=env_seed)\n",
    "        for env_seed in seed\n",
    "    ])\n",
    "    return env\n",
    "\n",
    "\n",
    "N_ENVS = 8\n",
    "N_STEPS = 20\n",
    "env = make_env(nenvs=N_ENVS)\n",
    "obs = env.reset()\n",
    "#print(obs)\n",
    "N_ACTIONS = env.action_space.n\n",
    "OBS_SHAPE = obs.shape\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "assert obs.shape == (8, 16, 64, 64), obs.shape\n",
    "assert obs.dtype == np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class ObserwationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, state):\n",
    "        return torch.tensor(state).float().to(DEVICE) / 255.0\n",
    "\n",
    "env = ObserwationWrapper(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0'),\n",
       " array([ 0.01786326,  0.11979456, -0.14803306, -0.13378104,  0.09468035,\n",
       "        -0.11347174, -0.11465508, -0.00580869]),\n",
       " array([False, False, False, False, False, False, False, False]),\n",
       " ({'mirror1_normal': array([-1.11072771e-04,  7.07062351e-01, -7.07151200e-01]),\n",
       "   'mirror2_normal': array([-3.48948691e-09, -7.07062351e-01,  7.07151209e-01]),\n",
       "   'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [-1.57090486e-04  9.99999980e-01 -1.25639030e-04]',\n",
       "   'reflect_with_mirror2': 'center = [ -0.15707075  -0.12563903 -10.12562325], k = [-1.57095422e-04  2.46757156e-08  9.99999988e-01]',\n",
       "   'angle_between_beams': 0.000157095423319282,\n",
       "   'state_calc_time': 0.004989147186279297,\n",
       "   'fit_time': 0,\n",
       "   'imin': 38.80729361630281,\n",
       "   'imax': 363.31657100519163,\n",
       "   'proj_1': array([0., 0., 0.]),\n",
       "   'proj_2': array([-0.15866144, -0.12563878,  0.        ]),\n",
       "   'dist': 0.20238220557125236,\n",
       "   'visib': 0.8069883584112533},\n",
       "  {'mirror1_normal': array([-2.22116232e-05,  7.07062351e-01, -7.07151208e-01]),\n",
       "   'mirror2_normal': array([-8.88660326e-05, -7.06973482e-01,  7.07240050e-01]),\n",
       "   'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [-3.14139524e-05  9.99999992e-01 -1.25662719e-04]',\n",
       "   'reflect_with_mirror2': 'center = [ -0.03141     -0.12569035 -10.12564693], k = [-1.57081604e-04  2.51332343e-04  9.99999956e-01]',\n",
       "   'angle_between_beams': 0.00029638248836513667,\n",
       "   'state_calc_time': 0.0047800540924072266,\n",
       "   'fit_time': 0,\n",
       "   'imin': 100.08236930093638,\n",
       "   'imax': 302.04150802035537,\n",
       "   'proj_1': array([0., 0., 0.]),\n",
       "   'proj_2': array([-0.03300056, -0.12314545,  0.        ]),\n",
       "   'dist': 0.127490543894401,\n",
       "   'visib': 0.5022311534066312},\n",
       "  {'mirror1_normal': array([ 8.88611478e-05,  7.07106781e-01, -7.07106776e-01]),\n",
       "   'mirror2_normal': array([-6.66362646e-05, -7.07195633e-01,  7.07017915e-01]),\n",
       "   'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [1.25668639e-04 9.99999992e-01 1.57922653e-08]',\n",
       "   'reflect_with_mirror2': 'center = [ 1.25668641e-01  3.94703466e-06 -9.99998421e+00], k = [ 3.14188898e-05 -2.51323460e-04  9.99999968e-01]',\n",
       "   'angle_between_beams': 0.00025327974485139424,\n",
       "   'state_calc_time': 0.002173185348510742,\n",
       "   'fit_time': 0,\n",
       "   'imin': 80.60819625767513,\n",
       "   'imax': 321.5156763000643,\n",
       "   'proj_1': array([0., 0., 0.]),\n",
       "   'proj_2': array([ 0.12598283, -0.00250928,  0.        ]),\n",
       "   'dist': 0.12600781637857525,\n",
       "   'visib': 0.5990877351053018},\n",
       "  {'mirror1_normal': array([-6.66404523e-05,  7.07151209e-01, -7.07062348e-01]),\n",
       "   'mirror2_normal': array([ 4.44232459e-05, -7.07240055e-01,  7.06973481e-01]),\n",
       "   'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [-9.42379093e-05  9.99999988e-01  1.25672588e-04]',\n",
       "   'reflect_with_mirror2': 'center = [-0.09424975  0.12563508 -9.87431162], k = [-3.14100057e-05 -2.51324447e-04  9.99999968e-01]',\n",
       "   'angle_between_beams': 0.0002532796234315567,\n",
       "   'state_calc_time': 0.004334449768066406,\n",
       "   'fit_time': 0,\n",
       "   'imin': 82.08378703875498,\n",
       "   'imax': 320.0400855189691,\n",
       "   'proj_1': array([0., 0., 0.]),\n",
       "   'proj_2': array([-0.0945599 ,  0.12315343,  0.        ]),\n",
       "   'dist': 0.15526861177653498,\n",
       "   'visib': 0.5917487488785088},\n",
       "  {'mirror1_normal': array([-4.44309230e-05,  7.07173421e-01, -7.07040133e-01]),\n",
       "   'mirror2_normal': array([-3.48970614e-09, -7.07151209e-01,  7.07062351e-01]),\n",
       "   'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [-6.28288914e-05  9.99999980e-01  1.88499506e-04]',\n",
       "   'reflect_with_mirror2': 'center = [-0.06284074  0.18851135 -9.81146496], k = [-6.28338260e-05  6.28358017e-05  9.99999996e-01]',\n",
       "   'angle_between_beams': 8.886184511398921e-05,\n",
       "   'state_calc_time': 0.004515886306762695,\n",
       "   'fit_time': 0,\n",
       "   'imin': 15.888891143411191,\n",
       "   'imax': 386.23497010374535,\n",
       "   'proj_1': array([0., 0., 0.]),\n",
       "   'proj_2': array([-0.06345723,  0.18912786,  0.        ]),\n",
       "   'dist': 0.19948977207641572,\n",
       "   'visib': 0.9209751388831641},\n",
       "  {'mirror1_normal': array([ 8.88604500e-05,  7.07040135e-01, -7.07173416e-01]),\n",
       "   'mirror2_normal': array([ 4.44274335e-05, -7.07151209e-01,  7.07062350e-01]),\n",
       "   'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [ 1.25679496e-04  9.99999974e-01 -1.88479765e-04]',\n",
       "   'reflect_with_mirror2': 'center = [  0.12565582  -0.18841268 -10.18844426], k = [ 1.88525162e-04 -3.14135569e-04  9.99999933e-01]',\n",
       "   'angle_between_beams': 0.0003663644337246913,\n",
       "   'state_calc_time': 0.001924276351928711,\n",
       "   'fit_time': 0,\n",
       "   'imin': 131.52006605106067,\n",
       "   'imax': 270.60382059554416,\n",
       "   'proj_1': array([0., 0., 0.]),\n",
       "   'proj_2': array([ 0.1275766 , -0.19161324,  0.        ]),\n",
       "   'dist': 0.2301986551246832,\n",
       "   'visib': 0.3458728992806968},\n",
       "  {'mirror1_normal': array([ 2.22165083e-05,  7.07062351e-01, -7.07151208e-01]),\n",
       "   'mirror2_normal': array([-6.66509205e-05, -7.07128995e-01,  7.07084563e-01]),\n",
       "   'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [ 3.14208613e-05  9.99999992e-01 -1.25662719e-04]',\n",
       "   'reflect_with_mirror2': 'center = [  0.03141691  -0.125642   -10.12564693], k = [-6.28525795e-05 -1.88497531e-04  9.99999980e-01]',\n",
       "   'angle_between_beams': 0.0001987001923266653,\n",
       "   'state_calc_time': 0.001979827880859375,\n",
       "   'fit_time': 0,\n",
       "   'imin': 54.91909815713273,\n",
       "   'imax': 347.20476944063955,\n",
       "   'proj_1': array([0., 0., 0.]),\n",
       "   'proj_2': array([ 0.03078049, -0.12755066,  0.        ]),\n",
       "   'dist': 0.13121207515860495,\n",
       "   'visib': 0.726854819709105},\n",
       "  {'mirror1_normal': array([-6.66495244e-05,  7.06884602e-01, -7.07328887e-01]),\n",
       "   'mirror2_normal': array([ 6.66418480e-05, -7.07173421e-01,  7.07040132e-01]),\n",
       "   'reflect_with_mirror1': 'center = [    0. -1000.   -10.], k = [-9.42862679e-05  9.99999798e-01 -6.28309604e-04]',\n",
       "   'reflect_with_mirror2': 'center = [ -0.09422709  -0.6278058  -10.62791527], k = [ 2.76112612e-08 -8.16813996e-04  9.99999666e-01]',\n",
       "   'angle_between_beams': 0.0008168140870700401,\n",
       "   'state_calc_time': 0.0033025741577148438,\n",
       "   'fit_time': 0,\n",
       "   'imin': 200.11099067096015,\n",
       "   'imax': 202.0130031334994,\n",
       "   'proj_1': array([0., 0., 0.]),\n",
       "   'proj_2': array([-0.0942268 , -0.63648684,  0.        ]),\n",
       "   'dist': 0.6434237962587377,\n",
       "   'visib': 0.004729915378946899}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([0] * N_ENVS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization wrapper will subtract running mean from observations and rewards and divide \n",
    "the resulting quantities by the  running variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will need to define a model for training. We suggest that you use two separate networks: one for policy\n",
    "and another for value function. Each network should be a 3-layer MLP with 64 hidden units, $\\mathrm{tanh}$ \n",
    "activation function, kernel matrices initialized with orthogonal initializer with parameter $\\sqrt{2}$\n",
    "and biases initialized with zeros. \n",
    "\n",
    "Our policy distribution is going to be multivariate normal with diagonal covariance. \n",
    "The network from above will predict the mean, and the covariance should be represented by a single \n",
    "(learned) vector of size 6 (corresponding to the dimensionality of the action space from above). \n",
    "You should initialize this vector to zero and take the exponent of it to always\n",
    "have a non-negative quantity. \n",
    "\n",
    "Overall the model should return three things: predicted mean of the distribution, variance vector, \n",
    "value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def ortho_weights(shape, scale=1.):\n",
    "    \"\"\" PyTorch port of ortho_init from baselines.a2c.utils \"\"\"\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        flat_shape = shape[1], shape[0]\n",
    "    elif len(shape) == 4:\n",
    "        flat_shape = (np.prod(shape[1:]), shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    a = np.random.normal(0., 1., flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.transpose().copy().reshape(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        return torch.from_numpy((scale * q).astype(np.float32))\n",
    "    if len(shape) == 4:\n",
    "        return torch.from_numpy((scale * q[:, :shape[1], :shape[2]]).astype(np.float32))\n",
    "\n",
    "\n",
    "def atari_initializer(module):\n",
    "    \"\"\" Parameter initializer for Atari models\n",
    "    Initializes Linear, Conv2d, and LSTM weights.\n",
    "    \"\"\"\n",
    "    classname = module.__class__.__name__\n",
    "\n",
    "    if classname == 'Linear':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'Conv2d':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'LSTM':\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'weight_hh' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'bias' in name:\n",
    "                param.data.zero_()\n",
    "                \n",
    " \n",
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class AtariCNN(nn.Module):\n",
    "    def __init__(self, num_actions, state_shape):\n",
    "        \"\"\" Basic convolutional actor-critic network for Atari 2600 games\n",
    "        Equivalent to the network in the original DQN paper.\n",
    "        Args:\n",
    "            num_actions (int): the number of available discrete actions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels=state_shape[1], out_channels=32, kernel_size=8, stride=4),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "                                  nn.ReLU(),\n",
    "                                  Flatten())\n",
    "        \n",
    "        convw = conv2d_size_out(state_shape[2], kernel_size=8, stride=4)\n",
    "        convw = conv2d_size_out(convw, kernel_size=4, stride=2)\n",
    "        convw = conv2d_size_out(convw, kernel_size=3, stride=1)\n",
    "        \n",
    "        convh = conv2d_size_out(state_shape[3], kernel_size=8, stride=4)\n",
    "        convh = conv2d_size_out(convh, kernel_size=4, stride=2)\n",
    "        convh = conv2d_size_out(convh, kernel_size=3, stride=1)\n",
    "       \n",
    "        linear_input_size = convw * convh * 64\n",
    "        \n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(linear_input_size, 512),\n",
    "                                nn.ReLU())\n",
    "\n",
    "        self.pi = nn.Linear(512, num_actions)\n",
    "        self.v = nn.Linear(512, 1)\n",
    "        \n",
    "        #std = 0\n",
    "        #self.log_std = nn.Parameter(torch.ones(1, num_actions) * std)\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # parameter initialization\n",
    "        self.apply(atari_initializer)\n",
    "        self.pi.weight.data = ortho_weights(self.pi.weight.size(), scale=.01)\n",
    "        self.v.weight.data = ortho_weights(self.v.weight.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Module forward pass\n",
    "        Args:\n",
    "            x (Variable): convolutional input, shaped [batch_size x 4 x 84 x 84]\n",
    "        Returns:\n",
    "            pi (Variable): action probability logits, shaped [batch_size x self.num_actions]\n",
    "            v (Variable): value predictions, shaped [batch_size x 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        conv_out = self.conv(x)\n",
    "        fc_out = self.fc(conv_out)\n",
    "\n",
    "        mu = self.pi(fc_out)\n",
    "        #std = self.log_std.exp().expand_as(mu)\n",
    "        value = self.v(fc_out)\n",
    "\n",
    "        return mu, 0, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 64, 64])\n",
      "tensor(-0.0608, device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = AtariCNN(N_ACTIONS, OBS_SHAPE).to(DEVICE)\n",
    "s = env.reset()\n",
    "print(s.shape)\n",
    "mu, std, value = model(s)\n",
    "print(value[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will be wrapped by a `Policy`. The policy can work in two modes, but in either case \n",
    "it is going to return dictionary with string-type keys. The first mode is when the policy is \n",
    "used to sample actions for a trajectory which will later be used for training. In this case \n",
    "the flag `training` passed to `act` method is `False` and the method should return \n",
    "a `dict` with the following keys: \n",
    "\n",
    "* `\"actions\"`: actions to pass to the environment\n",
    "* `\"log_probs\"`: log-probabilities of sampled actions\n",
    "* `\"values\"`: value function $V^\\pi(s)$ predictions.\n",
    "\n",
    "We don't need to use the values under these keys for training, so all of them should be of type `np.ndarray`.\n",
    "\n",
    "When `training` is `True`, the model is training on a given batch of observations. In this\n",
    "case it should return a `dict` with the following keys\n",
    "\n",
    "* `\"distribution\"`: an instance of multivariate normal distribution (`torch.distributions.MultivariateNormal` or `tf.distributions.MultivariateNormalDiag`)\n",
    "* `\"values\"`: value function $V^\\pi(s)$ prediction.\n",
    "\n",
    "The distinction about the modes comes into play depending on where the policy is used: if it is called from `EnvRunner`, \n",
    "the `training` flag is `False`, if it is called from `PPO`, the `training` flag is `True`. These classed \n",
    "will be described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class Policy:\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    \n",
    "  def act(self, inputs, training = False, determ = False):\n",
    "    logits, _, values = self.model(inputs)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    if training:\n",
    "        return dist, values\n",
    "        \n",
    "    else:\n",
    "        if determ:\n",
    "            actions = dist.probs.argmax(dim=1, keepdim=True)\n",
    "        else:\n",
    "            actions = dist.sample()\n",
    "        return {\n",
    "            'actions': actions.detach().cpu().numpy(),\n",
    "            'log_probs': dist.log_prob(actions).detach(),\n",
    "            'values': values.detach().cpu().numpy().reshape(-1)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (8,)\n",
      "log_probs tensor(-2.0787, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "m = AtariCNN(N_ACTIONS, OBS_SHAPE).to(DEVICE)\n",
    "p = Policy(m)\n",
    "s = env.reset()\n",
    "act = p.act(s)\n",
    "a = act['actions']\n",
    "lp = act['log_probs']\n",
    "print('actions', act['actions'].shape)\n",
    "print('log_probs', lp[0])\n",
    "#env.step(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `EnvRunner` to perform interactions with an environment with a policy for a fixed number of timesteps. Calling `.get_next()` on a runner will return a trajectory &mdash; dictionary \n",
    "containing keys\n",
    "\n",
    "* `\"observations\"`\n",
    "* `\"rewards\"` \n",
    "* `\"resets\"`\n",
    "* `\"actions\"`\n",
    "* all other keys that you defined in `Policy`,\n",
    "\n",
    "under each of these keys there is a `np.ndarray` of specified length $T$ &mdash; the size of partial trajectory. \n",
    "\n",
    "Additionally, before returning a trajectory this runner can apply a list of transformations. \n",
    "Each transformation is simply a callable that should modify passed trajectory in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RL env runner \"\"\"\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EnvRunner:\n",
    "  \"\"\" Reinforcement learning runner in an environment with given policy \"\"\"\n",
    "  def __init__(self, env, policy, nsteps,\n",
    "               transforms=None, step_var=None):\n",
    "    self.env = env\n",
    "    self.policy = policy\n",
    "    self.nsteps = nsteps\n",
    "    self.transforms = transforms or []\n",
    "    self.step_var = step_var if step_var is not None else 0\n",
    "    self.state = {\"latest_observation\": self.env.reset()}\n",
    "\n",
    "\n",
    "  @property\n",
    "  def nenvs(self):\n",
    "    \"\"\" Returns number of batched envs or `None` if env is not batched \"\"\"\n",
    "    return getattr(self.env.unwrapped, \"nenvs\", None)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\" Resets env and runner states. \"\"\"\n",
    "    self.state[\"latest_observation\"] = self.env.reset()\n",
    "    self.policy.reset()\n",
    "\n",
    "  def get_next(self):\n",
    "    \"\"\" Runs the agent in the environment.  \"\"\"\n",
    "    trajectory = defaultdict(list, {\"actions\": []})\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    resets = []\n",
    "    self.state[\"env_steps\"] = self.nsteps\n",
    "\n",
    "    for i in range(self.nsteps):\n",
    "      observations.append(self.state[\"latest_observation\"])\n",
    "      act = self.policy.act(self.state[\"latest_observation\"])\n",
    "      if \"actions\" not in act:\n",
    "        raise ValueError(\"result of policy.act must contain 'actions' \"\n",
    "                         f\"but has keys {list(act.keys())}\")\n",
    "      for key, val in act.items():\n",
    "        trajectory[key].append(val)\n",
    "\n",
    "      obs, rew, done, _ = self.env.step(trajectory[\"actions\"][-1])\n",
    "      self.state[\"latest_observation\"] = obs\n",
    "      rewards.append(rew)\n",
    "      resets.append(done)\n",
    "      self.step_var += self.nenvs or 1\n",
    "\n",
    "      # Only reset if the env is not batched. Batched envs should auto-reset.\n",
    "      if not self.nenvs and np.all(done):\n",
    "        self.state[\"env_steps\"] = i + 1\n",
    "        self.state[\"latest_observation\"] = self.env.reset()\n",
    "\n",
    "    trajectory.update(observations=observations, rewards=rewards, resets=resets)\n",
    "    trajectory[\"state\"] = self.state\n",
    "\n",
    "    for transform in self.transforms:\n",
    "      transform(trajectory)\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsArray:\n",
    "  \"\"\" \n",
    "  Converts lists of interactions to ndarray.\n",
    "  \"\"\"\n",
    "  def __call__(self, trajectory):\n",
    "    # Modify trajectory inplace. \n",
    "    for k, v in filter(lambda kv: kv[0] != \"state\" and kv[0] != \"observations\" and kv[0] != \"log_probs\",\n",
    "                       trajectory.items()):\n",
    "      trajectory[k] = np.asarray(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to implement the following two transformations. \n",
    "\n",
    "The first is `GAE` that implements [Generalized Advantage Estimator](https://arxiv.org/abs/1506.02438).\n",
    "In it you should add two keys to the trajectory: `\"advantages\"` and `\"value_targets\"`. In GAE the advantages\n",
    "$A_t^{\\mathrm{GAE}(\\gamma,\\lambda)}$ are essentially defined as the exponential \n",
    "moving average with parameter $\\lambda$ of the regular advantages \n",
    "$\\hat{A}^{(n)}(s_t) = \\sum_{l=0}^{T-1} \\gamma^l r_{t+l} + \\gamma^{T} V^\\pi(s_{t+l}) - V^\\pi(s_t)$. \n",
    "The exact formula for the computation is the following\n",
    "\n",
    "$$\n",
    "A_t^{\\mathrm{GAE}(\\gamma,\\lambda)} = \\sum_{l=0}^{T-1} (\\gamma\\lambda)^l\\delta_{t + l}^V,\n",
    "$$\n",
    "where $\\delta_{t+l}^V = r_{t+l} + \\gamma V^\\pi(s_{t+l+1}) - V^\\pi(s_{t+l})$. You can look at the \n",
    "derivation (formulas 11-16) in the paper. Don't forget to reset the summation on terminal\n",
    "states as determined by the flags `trajectory[\"resets\"]`. You can use `trajectory[\"values\"]`\n",
    "to get values of all observations except the most recent which is stored under \n",
    " `trajectory[\"state\"][\"latest_observation\"]`. For this observation you will need to call the policy \n",
    " to get the value prediction.\n",
    "\n",
    "Once you computed the advantages, you can get the targets for training the value function by adding \n",
    "back values:\n",
    "$$\n",
    "\\hat{V}(s_{t+l}) = A_{t+l}^{\\mathrm{GAE}(\\gamma,\\lambda)} + V(s_{t + l}),\n",
    "$$\n",
    "where $\\hat{V}$ is a tensor of value targets that are used to train the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = np.concatenate((values, [next_value]))\n",
    "    #values = values + [next_value]\n",
    "    gae = 0\n",
    "    gaes = []\n",
    "    value_targets = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        gaes.insert(0, gae)\n",
    "        value_targets.insert(0, gae + values[step])\n",
    "    return np.asarray(gaes), np.asarray(value_targets)\n",
    "\n",
    "\n",
    "class GAE:\n",
    "  \"\"\" Generalized Advantage Estimator. \"\"\"\n",
    "  def __init__(self, policy, gamma=0.99, lambda_=0.95):\n",
    "    self.policy = policy\n",
    "    self.gamma = gamma\n",
    "    self.lambda_ = lambda_\n",
    "    \n",
    "  def __call__(self, trajectory):\n",
    "    next_value = self.policy.act(trajectory['state']['latest_observation'])['values']\n",
    "   # print(trajectory['values'][0], len(trajectory['values']))\n",
    "    values = trajectory['values']\n",
    "    env_steps = trajectory['state']['env_steps']\n",
    "    rewards = trajectory['rewards']\n",
    "    dones = trajectory['resets']\n",
    "    is_not_done = 1.0 - dones\n",
    "    \n",
    "    trajectory['advantages'], trajectory['value_targets'] = compute_gae(\n",
    "        next_value, rewards, is_not_done, values, self.gamma, self.lambda_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "  \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
    "  def __call__(self, trajectory):\n",
    "    # Modify trajectory inplace. \n",
    "    #<TODO: implement>\n",
    "    #'actions', 'logits', 'log_probs', 'values', 'observations', 'rewards', 'resets', 'state', 'value_targets'\n",
    "    #print(type(trajectory['observations']), trajectory['observations'].shape)    \n",
    "    trajectory['observations'] = torch.stack(trajectory['observations']).reshape(-1, 16, 64, 64)\n",
    "    trajectory['actions'] = trajectory['actions'].reshape(-1)\n",
    "    trajectory['log_probs'] = torch.stack(trajectory['log_probs']).reshape(-1)\n",
    "    trajectory['value_targets'] = trajectory['value_targets'].reshape(-1)\n",
    "    trajectory['advantages'] = trajectory['advantages'].reshape(-1)    \n",
    "    trajectory['resets'] = trajectory['resets'].reshape(-1)\n",
    "    trajectory['rewards'] = trajectory['rewards'].reshape(-1)\n",
    "    \n",
    "    #print(trajectory['observations'].shape, trajectory['value_targets'].shape, trajectory['log_probs'].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of PPO over simpler policy based methods like A2C is that it is possible\n",
    "to train on the same trajectory for multiple gradient steps. The following class wraps \n",
    "an `EnvRunner`. It should call the runner to get a trajectory, then return minibatches \n",
    "from it for a number of epochs, shuffling the data before each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectorySampler:\n",
    "    \"\"\" Samples minibatches from trajectory for a number of epochs. \"\"\"\n",
    "    def __init__(self, runner, num_epochs, num_minibatches, transforms=None):\n",
    "        self.runner = runner\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_minibatches = num_minibatches\n",
    "        self.transforms = transforms or []\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\" Returns next minibatch.  \"\"\"\n",
    "        for _ in range(self.num_epochs):\n",
    "            trajectory = self.runner.get_next()\n",
    "            for t in self.transforms:\n",
    "                t(trajectory)\n",
    "            #may be not correct, fix me\n",
    "            rand_id = np.random.permutation(N_STEPS * N_ENVS)\n",
    "                \n",
    "            yield {\n",
    "                'observations': torch.stack([trajectory['observations'][i] for i in rand_id]),\n",
    "                'actions': np.asarray([trajectory['actions'][i] for i in rand_id]), \n",
    "                'log_probs': torch.stack([trajectory['log_probs'][i] for i in rand_id]),\n",
    "                'value_targets': np.asarray([trajectory['value_targets'][i] for i in rand_id]),\n",
    "                'advantages': np.asarray([trajectory['advantages'][i] for i in rand_id]),\n",
    "                'rewards': np.asarray([trajectory['rewards'][i] for i in rand_id]),\n",
    "                'resets': np.asarray([trajectory['resets'][i] for i in rand_id])\n",
    "            }\n",
    "                    #state, action, old_log_probs, return_, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common trick to use with GAE is to normalize advantages, the following transformation does that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAdvantages:\n",
    "  \"\"\" Normalizes advantages to have zero mean and variance 1. \"\"\"\n",
    "  def __call__(self, trajectory):\n",
    "    adv = trajectory[\"advantages\"]\n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "    trajectory[\"advantages\"] = adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our PPO runner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ppo_runner(env, policy, num_runner_steps=N_STEPS,\n",
    "                    gamma=0.99, lambda_=0.95, \n",
    "                    num_epochs=1, num_minibatches=32):\n",
    "    \"\"\" Creates runner for PPO algorithm. \"\"\"\n",
    "    #num_runner_steps was 2048\n",
    "    env.reset()\n",
    "\n",
    "    runner_transforms = [AsArray(),\n",
    "                         GAE(policy, gamma=gamma, lambda_=lambda_),\n",
    "                         MergeTimeBatch()]\n",
    "    runner = EnvRunner(env, policy, num_runner_steps, \n",
    "                       transforms=runner_transforms)\n",
    "  \n",
    "    sampler_transforms = [NormalizeAdvantages()]\n",
    "    sampler = TrajectorySampler(runner, num_epochs=num_epochs, \n",
    "                                num_minibatches=num_minibatches,\n",
    "                                transforms=sampler_transforms)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n",
      "tensor(-2.0790, device='cuda:0')\n",
      "{'observations': torch.Size([160, 16, 64, 64]), 'actions': (160,), 'log_probs': torch.Size([160]), 'value_targets': (160,), 'advantages': (160,), 'rewards': (160,), 'resets': (160,)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = AtariCNN(N_ACTIONS, OBS_SHAPE).to(DEVICE)\n",
    "p = Policy(a)\n",
    "r = make_ppo_runner(env, p)\n",
    "trajectory = r.get_next()\n",
    "\n",
    "\n",
    "for minibatch in trajectory:\n",
    "    print(minibatch['observations'][0].type())\n",
    "    print(minibatch['log_probs'][0])\n",
    "    p.act(minibatch['observations'])\n",
    "    print({k: v.shape for k, v in minibatch.items() if k != \"state\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you will need to implement Proximal Policy Optimization algorithm itself. The algorithm\n",
    "modifies the typical policy gradient loss in the following way:\n",
    "\n",
    "$$\n",
    "L_{\\pi} = \\frac{1}{T-1}\\sum_{l=0}^{T-1}\n",
    "\\frac{\\pi_\\theta(a_{t+l}|s_{t+l})}{\\pi_\\theta^{\\text{old}}(a_{t+l}|s_{t+l})}\n",
    "A^{\\mathrm{GAE}(\\gamma,\\lambda)}_{t+l}\\\\\n",
    "L_{\\pi}^{\\text{clipped}} = \\frac{1}{T-1}\\sum_{l=0}^{T-1}\\mathrm{clip}\\left(\n",
    "\\frac{\\pi_\\theta(a_{t+l}|s_{t+l})}{\\pi_{\\theta^{\\text{old}}}(a_{t+l}|s_{t+l})}\n",
    "\\cdot A^{\\mathrm{GAE(\\gamma, \\lambda)}}_{t+l},\n",
    "1 - \\text{cliprange}, 1 + \\text{cliprange}\\right)\\\\\n",
    "L_{\\text{policy}} = \\max\\left(L_\\pi, L_{\\pi}^{\\text{clipped}}\\right).\n",
    "$$\n",
    "\n",
    "Additionally, the value loss is modified in the following way:\n",
    "\n",
    "$$\n",
    "L_V = \\frac{1}{T-1}\\sum_{l=0}^{T-1}(V_\\theta(s_{t+l}) - \\hat{V}(s_{t+l}))^2\\\\\n",
    "L_{V}^{\\text{clipped}} = \\frac{1}{T-1}\\sum_{l=0}^{T-1}\n",
    "V_{\\theta^{\\text{old}}}(s_{t+l}) +\n",
    "\\text{clip}\\left(\n",
    "V_\\theta(s_{t+l}) - V_{\\theta^\\text{old}}(s_{t+l}),\n",
    "-\\text{cliprange}, \\text{cliprange}\n",
    "\\right)\\\\\n",
    "L_{\\text{value}} = \\max\\left(L_V, L_V^{\\text{clipped}}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "  def __init__(self, policy, optimizer,\n",
    "               cliprange=0.2,\n",
    "               value_loss_coef=0.25,\n",
    "               entropy_coef=0.01,\n",
    "               max_grad_norm=0.5):\n",
    "    self.policy = policy\n",
    "    self.optimizer = optimizer\n",
    "    self.cliprange = cliprange\n",
    "    self.value_loss_coef = value_loss_coef\n",
    "    self.entropy_coef=entropy_coef\n",
    "    self.max_grad_norm = max_grad_norm\n",
    "    \n",
    "    self.ploss = None\n",
    "    self.vloss = None\n",
    "    self.ppo_loss = None\n",
    "    \n",
    "    self.vtargets = None\n",
    "    self.values = None\n",
    "    self.advantage = None\n",
    "    self.entropy = None\n",
    "    \n",
    "    self.grad_norm = None\n",
    "    \n",
    "  def policy_loss(self, trajectory, dist, value):\n",
    "    \"\"\" Computes and returns policy loss on a given trajectory. \"\"\"\n",
    "    advantages = torch.tensor(trajectory['advantages'], dtype=torch.float32).to(DEVICE)\n",
    "    old_log_probs = torch.tensor(trajectory['log_probs'], dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    a = torch.tensor(trajectory['actions']).to(DEVICE)\n",
    "    new_log_probs = dist.log_prob(a)\n",
    "    ratio = (new_log_probs - old_log_probs).exp()\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1.0 - self.cliprange, 1.0 + self.cliprange) * advantages\n",
    "    actor_loss  = - torch.min(surr1, surr2).mean() - self.entropy_coef * dist.entropy().mean()\n",
    "    \n",
    "    self.ploss = actor_loss.item()\n",
    "    self.advantage = torch.mean(advantages).item()\n",
    "    self.entropy = torch.mean(dist.entropy().mean()).item()\n",
    "    \n",
    "    return actor_loss\n",
    "      \n",
    "  def value_loss(self, trajectory, dist, value):\n",
    "    \"\"\" Computes and returns value loss on a given trajectory. \"\"\"\n",
    "    vtargets = torch.tensor(trajectory['value_targets'], dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    self.vtargets = np.mean(trajectory['value_targets'])\n",
    "    self.values = torch.mean(value).item()\n",
    "    \n",
    "    critic_loss = (vtargets - value).pow(2).mean()\n",
    "    \n",
    "    self.vloss = critic_loss.item()\n",
    "    \n",
    "    return critic_loss\n",
    "      \n",
    "  def loss(self, trajectory):\n",
    "    dist, value = self.policy.act(trajectory[\"observations\"], training=True)\n",
    "    total_loss = \\\n",
    "        self.policy_loss(trajectory, dist, value) + \\\n",
    "        self.value_loss_coef * self.value_loss(trajectory, dist,value)\n",
    "    self.ppo_loss = total_loss.item()\n",
    "    return total_loss\n",
    "      \n",
    "  def step(self, trajectory):\n",
    "    \"\"\" Computes the loss function and performs a single gradient step. \"\"\"\n",
    "    self.optimizer.zero_grad()\n",
    "    self.loss(trajectory).backward()\n",
    "    self.grad_norm = nn.utils.clip_grad_norm_(policy.model.parameters(), self.max_grad_norm)\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to do training. In one million of interactions it should be possible to \n",
    "achieve the total raw reward of about 1500. You should plot this quantity with respect to \n",
    "`runner.step_var` &mdash; the number of interactions with the environment. It is highly \n",
    "encouraged to also provide plots of the following quantities (these are useful for debugging as well):\n",
    "\n",
    "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) between \n",
    "value targets and value predictions\n",
    "* Entropy of the policy $\\pi$\n",
    "* Value loss\n",
    "* Policy loss\n",
    "* Value targets\n",
    "* Value predictions\n",
    "* Gradient norm\n",
    "* Advantages\n",
    "\n",
    "For optimization it is suggested to use Adam optimizer with linearly annealing learning rate \n",
    "from 3e-4 to 0 and epsilon 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def evaluate(policy, env, n_games=10):\n",
    "    n_solved = 0\n",
    "    n_steps = 0\n",
    "    dist = 0\n",
    "    angle = 0\n",
    "    for i in range(n_games):\n",
    "        s = env.reset()\n",
    "        istep = 0\n",
    "        while(True):\n",
    "            s = s.reshape(1, 16, 64, 64)\n",
    "            action = policy.act(s, determ=True)['actions'][0]\n",
    "            s, reward, done, info = env.step(action)\n",
    "            istep += 1\n",
    "            if done:\n",
    "                n_solved += istep < 200\n",
    "                n_steps += istep\n",
    "                dist += info['dist']\n",
    "                angle += info['angle_between_beams']\n",
    "                break\n",
    "    return n_solved / n_games, n_steps / n_games, dist / n_games, angle / n_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62500 [00:00<?, ?it/s]/home/dmitry/.local/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  2%|▏         | 1219/62500 [23:49<5:38:02,  3.02it/s] Process Process-9:\n",
      "Process Process-13:\n",
      "Process Process-10:\n",
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process Process-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process Process-14:\n",
      "KeyboardInterrupt\n",
      "Process Process-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-16:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dmitry/dev/agents/ppo/env_batch.py\", line 121, in worker\n",
      "    cmd, action = worker_connection.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8ea0af61fcc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mn_solved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval_solved_games'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_solved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-308163020fc0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(policy, env, n_games)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mistep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/gym_interf/gym_interf/envs/interf_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mcenter1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave_vector1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave_vector2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtot_intens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave_vector1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave_vector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_projection_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave_vector1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwave_vector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/gym_interf/gym_interf/envs/interf_env.py\u001b[0m in \u001b[0;36m_calc_state\u001b[0;34m(self, center1, wave_vector1, center2, wave_vector2)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mwave_vector2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterfEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mInterfEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterfEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterfEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momega\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             has_interf=has_interf)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mtend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/gym_interf/gym_interf/envs/calc_image_cpp.py\u001b[0m in \u001b[0;36mcalc_image\u001b[0;34m(start, end, n_points, wave_vector1, center1, radius1, wave_vector2, center2, radius2, n_frames, lamb, omega, has_interf, n_threads)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mto_double_pointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave_vector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_double_pointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mn_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_interf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mn_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_intens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\n",
    "    'runs/ppo_fixed3')\n",
    "\n",
    "agent = AtariCNN(N_ACTIONS, OBS_SHAPE).to(DEVICE)\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=3e-4, eps=1e-5)\n",
    "policy = Policy(agent)\n",
    "\n",
    "evaluate_env = ObserwationWrapper(make_interf_env(1234))\n",
    "ppo = PPO(policy, opt)\n",
    "runner = make_ppo_runner(\n",
    "    ObserwationWrapper(make_env(nenvs=N_ENVS)), \n",
    "    policy)\n",
    "\n",
    "entropis = []\n",
    "vlosses = []\n",
    "plosses = []\n",
    "vtargets = []\n",
    "vpredictions = []\n",
    "grad_norms = []\n",
    "advantages = []\n",
    "ppo_losses = []\n",
    "rewards = np.zeros(N_ENVS, dtype=float)\n",
    "dones = np.zeros(N_ENVS, dtype=float)\n",
    "steps = np.zeros(N_ENVS, dtype=float)\n",
    "\n",
    "eval_solved_games = []\n",
    "mean_eval_steps = []\n",
    "n_frames = []\n",
    "\n",
    "\n",
    "for i in trange(0, int(1e7), N_ENVS * N_STEPS):\n",
    "    for trajectory in runner.get_next():\n",
    "        ppo.step(trajectory)\n",
    "    \n",
    "        for batch_rewards, batch_dones in zip(trajectory['rewards'], trajectory['resets']):\n",
    "            rewards += batch_rewards\n",
    "            dones += batch_dones\n",
    "            steps += 1\n",
    "\n",
    "        entropis.append(ppo.entropy)\n",
    "        vlosses.append(ppo.vloss)\n",
    "        plosses.append(ppo.ploss)\n",
    "        vtargets.append(ppo.vtargets)\n",
    "        vpredictions.append(ppo.values)\n",
    "        grad_norms.append(ppo.grad_norm)\n",
    "        advantages.append(ppo.advantage)\n",
    "        ppo_losses.append(ppo.ppo_loss)\n",
    "\n",
    "        if np.sum(dones) >= 100:\n",
    "            writer.add_scalar('entropy', np.mean(entropis), i)\n",
    "            writer.add_scalar('vloss', np.mean(vlosses), i)\n",
    "            writer.add_scalar('ploss', np.mean(plosses), i)\n",
    "            writer.add_scalar('vtarget', np.mean(vtargets), i)\n",
    "            writer.add_scalar('vprediction', np.mean(vpredictions), i)\n",
    "            writer.add_scalar('grad_norm', np.mean(grad_norms), i)\n",
    "            writer.add_scalar('advantage', np.mean(advantages), i)\n",
    "            writer.add_scalar('loss', np.mean(ppo_losses), i)\n",
    "            writer.add_scalar('reward', np.mean(rewards / dones), i)\n",
    "            writer.add_scalar('steps', np.mean(steps / dones), i)\n",
    "\n",
    "            n_solved, n_steps, dist, angle = evaluate(policy, evaluate_env)\n",
    "\n",
    "            writer.add_scalar('eval_solved_games', n_solved, i)\n",
    "            writer.add_scalar('eval_steps', n_steps, i)\n",
    "            writer.add_scalar('dist_between_beams', dist, i)\n",
    "            writer.add_scalar('angle_between_beams', angle, i)\n",
    "\n",
    "            entropis = []\n",
    "            vlosses = []\n",
    "            plosses = []\n",
    "            vtargets = []\n",
    "            vpredictions = []\n",
    "            grad_norms = []\n",
    "            advantages = []\n",
    "            ppo_losses = []\n",
    "            rewards = np.zeros(N_ENVS, dtype=float)\n",
    "            dones = np.zeros(N_ENVS, dtype=float)\n",
    "            steps = np.zeros(N_ENVS, dtype=float)\n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
